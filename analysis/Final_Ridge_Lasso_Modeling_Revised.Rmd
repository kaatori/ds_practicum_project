---
title: "Ridge & Lasso Modeling of Clade C Bacteria"
author: "Cassandra Sperow"
date: "2023-11-14"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
suppressMessages(library(tidyverse))
```

#### Note: **Revised** to make sure to use ```cv.glmnet()``` with K-fold cross-validation when fitting the ridge and lasso models
#### The modeling that includes **other variables** with the ASVs returns **better error rates. This section starts with the heading 'Other variables'**. Feel free to skip to that section.

# Review Goal & Question(s)
- Can Clade C bacteria be modeled using classification algorithms?
- When Clade C bacteria are modeled, which bacteria strains (ASVs) seem to be the 'best', or most associated, predictors of Clade C?

# Import Logistic Regression Balanced Dataset
- Clade C: n=139
- Non-Clade C: n=139 (mostly Clade A)
- Use first 1000 ASVs obtained in PCA Dim.1
- The ```lr_data.csv``` dataset was created by subsetting the Clade C observations from the original 659 samples with random sampling from non-Clade C rows to obtain an equal number of observations for each response label of Clade C = 1, Non-Clade C = 0. This data subsetting process was obtained in the original modeling exploration file 'Experimental Logistic Regression Clade C.Rmd'. 
- The Clade C Principal Components Analysis results from dimension 1 were obtained in the file 'PCA-CLR.Rmd'.
- The ASV data are transformed for modeling with the centered log ratio of the raw abundance counts. This is redone when subsetting for the PCA 1000 results and again when subsetting the first 500 PCA results, and again for the first 200 given that the ratios are dependent. 

```{r}
# read in balanced Clade C and non-Clade C observations 
# this was created from balanced df from previous file, but only has the ASV columns, Clade and Sample ID
read_csv( "../output/lr_data.csv") -> lr_data
# read in saved PCA ranks for Dimension 1
read_csv("../output/clade_c_pca_Ranks.csv") -> clade_c_pca_Ranks

```

```{r}
# order the Clade C ASVs by selecting from the vector of PCA results
# this is for subsetting the first 1000, 500, 200 in order of their PCA rank
lr_data %>% 
  select(sample_id, Clade, (clade_c_pca_Ranks$ASV)) %>% 
  select(1:1002) -> lr_data_1000

dim(lr_data_1000)
```

- Note that Clade as the Y response is a double as glmnet expects a numeric 0/1 for classification.
```{r}
#glimpse(lr_data)
head(lr_data_1000)
```

### Centered-Log-Ratio Data Transformation of 1000 ASV Numeric Columns
```{r}
library(compositions)
# CLR Data Transformation on only ASV numeric columns
bind_cols(lr_data_1000[,1:2], # sample id and Clade columns
          # re-do CLR transformation on relative first 1000 PCA
          lr_data_1000[,-c(1,2)] %>% clr() 
            ) -> lr_clr_df_1000

```

```{r}
head(lr_clr_df_1000)
#lr_clr_df_1000$ASV0001[1:5]
```



# Ridge 1000 

 ```glmnet``` documentation:

https://glmnet.stanford.edu/articles/glmnet.html

**cv.glmnet()** performs K-fold cross-validation without the need for separating training/testing.

```{r}
library(glmnet)
# create model matrix for glmnet
#stats::model.matrix()

# Create a formula for model.matrix
formula <- as.formula(paste("Clade ~ .", sep = "", collapse = ""), 
                      env = environment(lr_clr_df_1000))

# Create the design matrix
# take out the sample id
X <- stats::model.matrix(formula, data = lr_clr_df_1000[,-c(1)])
dim(X)

y <- lr_clr_df_1000$Clade
length(y)


# Train the RIDGE model using K-fold cross-validation
set.seed(23)
RidgeCV_1000 <- cv.glmnet(X[,-1], # take out intercept
             y, 
             alpha = 0,  # RIDGE
             type.measure = "class", # misclassification error
             nfolds = 10,
             family = "binomial"
             )
```

### Ridge 1000 CV Plot
```{r}
base::plot(RidgeCV_1000)
```

-  Access the error rates for lambdas
```{r}
RidgeCV_1000
#min(RidgeCV_1000$cvm) # from lambda.min 

```

```{r}
# best fitted model from CV with minimum error rate
coef(RidgeCV_1000, s = "lambda.min") %>% 
  as.matrix() %>% 
  as.data.frame() %>% 
  # order in terms of magnitude although beware ridge uses bias constraint
  # the magnitude should be interpreted in terms of the coef that were NOT shrunk as fast to approach 0
  arrange(-abs(s1)) %>% 
  head()
```

# Lasso 1000
- Use same design matrix as above with 1000 ASV columns
```{r}
dim(X)
length(y)
set.seed(123)
lasso_cv <- cv.glmnet(X[,-1], 
                   y, 
                   alpha = 1, # Lasso
                   type.measure = "class", # must have for classification
                   family = "binomial" # classification
                   )
lasso_cv

base::plot(lasso_cv)

# best lasso models from CV
lasso_cv
```


# Ridge 500 
### Data Prep
- Subset from original counts for the first 500 ASVs from PCA results
```{r}
# subset the first 500 
lr_data_1000[, 1:502] -> lr_data_500

# redo CLR transformation with counts for first 500
bind_cols(lr_data_500[,1:2], # 278 by 2
          lr_data_500[,-c(1,2)] %>% clr() # 278 by 500
            ) -> lr_clr_500
```


### Design matrix for 500 columns
```{r}
# Create a formula for model.matrix
formula <- as.formula(paste("Clade ~ .", sep = "", collapse = ""), 
                      env = environment(lr_clr_500))

# Create the design matrix
# take out the sample id
X <- stats::model.matrix(formula, data = lr_clr_500[,-c(1)])
dim(X)

y <- lr_clr_500$Clade
length(y)
```


#### Ridge CV 500
```{r}
set.seed(42)
# Train the RIDGE model with 500 ASVs
RidgeCV_500 <- cv.glmnet(X[,-1], # take out intercept
                 y, 
                 alpha = 0,  # RIDGE
                 nfolds = 10,
                 type.measure = "class", # misclassification error
                 family = "binomial") 

# best lambda
base::plot(RidgeCV_500)
```


```{r}
RidgeCV_500
```


```{r}
coef(RidgeCV_500, s = "lambda.min") %>% 
  as.matrix() %>% 
  as.data.frame() %>% 
  # ordering in terms of magnitude
  arrange(-abs(s1)) %>% 
  head()
```

# What if it was 200 ASVs?
- PCA from Clade C had about 150 or less that created a plateau in one of the plots for PCA top contributing ASVs. See the Rmd file for PCA. 
```{r}
# subset the first 200
lr_data_1000[, 1:202] -> lr_data_200

# redo CLR transformation with counts for first 200
bind_cols(lr_data_200[,1:2], #  278 by 2
          lr_data_200[,-c(1,2)] %>% clr() # 278 by 200
            ) -> lr_clr_200

####################################### Design Matrix
# Create a formula for model.matrix
formula <- as.formula(paste("Clade ~ .", sep = "", collapse = ""), 
                      env = environment(lr_clr_200))

# Create the design matrix
# take out the sample id
X <- stats::model.matrix(formula, data = lr_clr_200[,-c(1)])
dim(X)

y <- lr_clr_200$Clade
length(y)

##################################### CV for 200 ASVs
set.seed(23)
RidgeCV_200 <- cv.glmnet(X[,-1], # take out intercept
                 y, 
                 alpha = 0,  # RIDGE
                 nfolds = 10,
                 type.measure = "class", # misclassification error
                 family = "binomial") 

# best lambda
base::plot(RidgeCV_200)

```

```{r}
RidgeCV_200
```

- Is the error rate going up because maybe fewer ASVs make the data even more sparse?

- It does not seem to be helping to redo with cv.glmnet when the error rate is going up. What if non-ASV variables are included? Would the error rates go down? 


# Other Variables - Revised & Final Modeling Here
- Including other variables from original data to see if it can give better error rates
- This is the section mentioned at the very beginning where the reader may use as a starting point to review revsied results.
```{r redo-data-set}
library(compositions)
# read in data with other variables 
read_csv("../output/balanced_df.csv") %>% 
  # take out reef because it is not specific
  # take out Majority because it is used to define Clade C - as well as ITS2 Type - which is subcategory of clade
  select(-reef, -Majority, -ITS2_type) -> balanced_w_other_vars

# make dataset for 1000 ASVs 
balanced_w_other_vars %>% 
  # reorder for only CLade C PCA ranks of ASVs
  select(Clade, sample_id, its2_count, species, region, c(clade_c_pca_Ranks$ASV[1:1000])) %>% 
  mutate(Clade = ifelse(Clade=="C", 1, 0)) -> balanced_1000
# clr for 1000
bind_cols(
  balanced_1000[,1:5],
  compositions::clr(balanced_1000[,6:1005]) 
) -> clr_1000


head(clr_1000)
dim(clr_1000)

write_csv(clr_1000, "../output/clr_1000.csv")

# make dataset for the 500 ASVs
balanced_w_other_vars %>% 
  select(Clade, sample_id, its2_count, species, region, c(clade_c_pca_Ranks$ASV[1:500])) %>% 
  mutate(Clade = ifelse(Clade=="C", 1, 0)) -> balanced_500
# clr for 500
bind_cols(
  balanced_500[,1:5],
  clr(balanced_500[,6:505])
) -> clr_500

head(clr_500)

write_csv(clr_500, "../output/clr_500.csv")



# make another for the first 200 ASVs
balanced_w_other_vars %>% 
  select(Clade, sample_id, its2_count, species, region, c(clade_c_pca_Ranks$ASV[1:200])) %>% 
  mutate(Clade = ifelse(Clade=="C", 1, 0)) -> balanced_200
# clr for 200
bind_cols(
  balanced_500[,1:5],
  clr(balanced_500[,6:205])
) -> clr_200

head(clr_200)
write_csv(clr_200, "../output/clr_200.csv")

```

# Ridge 1000
```{r}
# design matrix
library(glmnet)
# Create a formula for model.matrix
formula <- as.formula(paste("Clade ~ .", sep = "", collapse = ""), 
                      env = environment(clr_1000))

X <- stats::model.matrix(formula, data = clr_1000[,-c(2)]) # minus sample id
dim(X)
#X[,1:10]

y <-  clr_1000$Clade
length(y)

# Fit the RIDGE model using K-fold cross-validation
set.seed(23)
R_clr_1000 <- cv.glmnet(X[,-1], # take out intercept
             y, 
             alpha = 0,  # RIDGE
             type.measure = "class", # misclassification error
             nfolds = 10,
             family = "binomial"
             )

# call
R_clr_1000 # 28.4 % with lambda min

# plot
base::plot(R_clr_1000)

# coef
coef(R_clr_1000, s = "lambda.min") %>% 
  as.matrix() %>% 
  as.data.frame() %>% 
  arrange(-abs(s1)) %>% 
  head()
```

# Ridge 500
```{r}
# Create a formula for model.matrix
formula <- as.formula(paste("Clade ~ .", sep = "", collapse = ""), 
                      env = environment(clr_500))

X <- stats::model.matrix(formula, data = clr_500[,-c(2)]) # minus sample id
dim(X)
# X[,1:10]

y <-  clr_500$Clade
length(y)

# Fit the RIDGE model using K-fold cross-validation
set.seed(23)
R_clr_500 <- cv.glmnet(X[,-1], # take out intercept
             y, 
             alpha = 0,  # RIDGE
             type.measure = "class", # misclassification error
             nfolds = 10,
             family = "binomial"
             )

# call
R_clr_500 #    % with lambda min

# plot
base::plot(R_clr_500)

# coef
coef(R_clr_500, s = "lambda.min") %>% 
  as.matrix() %>% 
  as.data.frame() %>% 
  arrange(-abs(s1)) %>% 
  head()
```

# Ridge 200 
```{r}
library(glmnet)
# Create a formula for model.matrix
formula <- as.formula(paste("Clade ~ .", sep = "", collapse = ""), 
                      env = environment(clr_200)) # 200

X <- stats::model.matrix(formula, data = clr_200[,-c(2)]) # minus sample id for 200 ASVs with other vars
dim(X)
# X[,1:10]

y <-  clr_200$Clade # 200
length(y)

# Fit the RIDGE model using K-fold cross-validation
set.seed(23)
R_clr_200 <- cv.glmnet(X[,-1], # take out intercept
             y, 
             alpha = 0,  # RIDGE
             type.measure = "class", # misclassification error
             nfolds = 10,
             family = "binomial"
             )

# call
R_clr_200 #    % with lambda min

# plot
base::plot(R_clr_200)

# coef
coef(R_clr_200, s = "lambda.min") %>% 
  as.matrix() %>% 
  as.data.frame() %>% 
  arrange(-abs(s1)) %>% 
  head()

```

- Wow, the error rate went down drastically to 16.2 % for lambda.min with a standard error for lambda.min that is slightly larger. This was using more than just the ASV columns. The model includes region, species, and ITS2_count. 

```{r}
# save modeling results
write_rds(R_clr_200, "../output/ridge_200_model.rds")
```


# Lasso Modeling with other variables included
# Lasso 1000
```{r}
# design matrix
library(glmnet)
# Create a formula for model.matrix
formula <- as.formula(paste("Clade ~ .", sep = "", collapse = ""), 
                      env = environment(clr_1000))

X <- stats::model.matrix(formula, data = clr_1000[,-c(2)]) # minus sample id
dim(X)
# X[,1:10]

y <-  clr_1000$Clade
length(y)

############### Fit the LASSO model using K-fold cross-validation
set.seed(23)
L_clr_1000 <- cv.glmnet(X[,-1], # take out intercept
             y, 
             alpha = 1,  # LASSO alpha = 1 
             type.measure = "class", # misclassification error
             nfolds = 10,
             family = "binomial"
             )

# call
L_clr_1000 #   % with lambda min

# plot
base::plot(L_clr_1000)

# coef
coef(L_clr_1000, s = "lambda.min") %>% 
  as.matrix() %>% 
  as.data.frame() %>% 
  filter(s1 != 0) %>% 
  arrange(-abs(s1)) %>% 
  head()
```

# Lasso 500
```{r}
# Create a formula for model.matrix
formula <- as.formula(paste("Clade ~ .", sep = "", collapse = ""), 
                      env = environment(clr_500))

X <- stats::model.matrix(formula, data = clr_500[,-c(2)]) # minus sample id
dim(X)
# X[,1:10]

y <-  clr_500$Clade
length(y)

############### Fit the LASSO model using K-fold cross-validation
set.seed(23)
L_clr_500 <- cv.glmnet(X[,-1], # take out intercept
             y, 
             alpha = 1,  # LASSO alpha = 1 
             type.measure = "class", # misclassification error
             nfolds = 10,
             family = "binomial"
             )

# call
L_clr_500 #   % with lambda min

# plot
base::plot(L_clr_500)

# coef
coef(L_clr_500, s = "lambda.min") %>% 
  as.matrix() %>% 
  as.data.frame() %>% 
  filter(s1 != 0) %>% 
  arrange(-abs(s1)) %>% 
  head()
```

# Lasso 200
```{r}
# Create a formula for model.matrix
formula <- as.formula(paste("Clade ~ .", sep = "", collapse = ""), 
                      env = environment(clr_200))

X <- stats::model.matrix(formula, data = clr_200[,-c(2)]) # minus sample id
dim(X)
# X[,1:10]

y <-  clr_200$Clade
length(y)

############### Fit the LASSO model using K-fold cross-validation
set.seed(23)
L_clr_200 <- cv.glmnet(X[,-1], # take out intercept
             y, 
             alpha = 1,  # LASSO alpha = 1 
             type.measure = "class", # misclassification error
             nfolds = 10,
             family = "binomial", 
             maxit = 10^7.5 ##### this is needed because of a warning message where it ran out before doing 100 lambda values
             ## increasing it several times over the default still did not give the full 100 lambda values, but 96 out of 100. 
             )

# call
L_clr_200 #   % with lambda min

# plot
base::plot(L_clr_200)

# coef
coef(L_clr_200, s = "lambda.min") %>% 
  as.matrix() %>% 
  as.data.frame() %>% 
  filter(s1 != 0) %>% 
  arrange(-abs(s1)) %>% 
  head()
```
- The lambda.min index is 37. The warning message is for not getting to go through the default of 100 lambda values. Looking at the plot of lambda values, glmnet is reporting the min at index 37, and the 1se at index 3. These low index values and the fact that the plot indicates the variance starts to increase again gives the impression that this warning message might be ok, at least for the short term. If these same variables need to be used to build a model, other optimization methods would be necessary to ensure convergence to the best minimum. 


# Overall Rates for Models with Other Variables

```{r}
#min(R_clr_1000$cvm)

# save outputs into list for using map()
list(
R_clr_1000,
R_clr_500,
R_clr_200) -> list_ridge_models

map(list_ridge_models, ~min(.$cvm)) %>% 
  as_vector() -> ridge_error_rates

# save outputs into list for using map()
list(L_clr_1000,
L_clr_500,
L_clr_200
) -> list_lasso_models

# map
map(list_lasso_models, ~min(.$cvm)) %>% 
  as_vector() -> lasso_error_rates

model_names <- c("ridge 1000", "ridge 500", "ridge 200", "lasso 1000", "lasso 500", "lasso 200*")

# combine to have them all in ore place for comparison
cbind(
  model_names,
c(ridge_error_rates, lasso_error_rates)
) %>% 
  as.data.frame() %>% 
 # mutate(error_rates = V2) %>% 
  mutate(error_rates_pct = as.numeric(V2)*100, .keep = "unused") %>% 
  arrange(error_rates_pct) %>% 
  mutate(row_id = row_number())

```

- The models with other variables perform better overall. The lowest error rate obtained in the first 'Ridge & Lasso Modeling.Rmd' file was 29 %; however, including the non-ASV variables of species, algae count (ITS2_count), and region provide a drastic improvement. Several revised models with error rates around 15-16 % were obtained in the latter modeling section of 'Other Variables'. 

- For the variables that were selected from the lasso model with the lowest error rate, the recommendation would be to see if the variables selected make biologic sense for the client as the subject matter expert, for example, if the client is familiar with any of the ASVs that were selected as predictors. These are listed below for all of the lasso models with other variables included because each of them had an error rate very similar.

If the objective later is to use any of these models in production or research for predictions, the recommendation would be to use the less complex ridge method with 200 ASVs with other variables included. Also the lasso method with 1000 ASV variables with other variables of species, region, and ITS2_count) could be considered, although normally it's best to go with fewer predictors when the error rates are so close (as in this case between lasso 1000 and ridge 200). It's less computationally expensive and provides better interpretability.

* The asterisk by the lasso 200 model is due to the fact that a warning message appeared relating to iterating over 96/100 lambda values. This model was re-attempted with increasing the max iterations to give more time for convergence; however, this was not successful. As this model came in 3rd overall for error rates in this analysis, it is recommended to consider the first two models mentioned above. 

```{r}
L_clr_1000
L_clr_500
L_clr_200

map(list_lasso_models, ~coef(., s = "lambda.min") %>% as.matrix(.) %>% 
      as.data.frame() %>% 
      mutate(predictor = rownames(.)) %>% 
  filter(s1 != 0) %>% 
  arrange(-abs(s1))) -> lasso_coefs


# test
#str(lasso_coefs[1])
# coef(L_clr_200, s = "lambda.min") %>% 
#   as.matrix() %>% 
#   as.data.frame() %>% 
#   filter(s1 != 0) %>% 
#   arrange(-abs(s1))

print('Lasso 1000 predictors arranged by absolute value of coefficient:')
lasso_coefs[1]
```

```{r}
# write all 3 model results to file
write_rds(lasso_coefs, "../output/lasso_coefs.rds")


```

### Region MAQ in all of Lasso Variable Selections
```{r}

clr_200 %>% 
  filter(Clade == "1") %>% 
  group_by(region) %>% 
  summarise(n = n()) %>% 
  arrange(-n)
```

```{r}
R_clr_1000
R_clr_500
R_clr_200

map(list_ridge_models, ~coef(., s = "lambda.min") %>% as.matrix(.) %>% 
      as.data.frame() %>% 
      mutate(predictor = rownames(.)) %>% 
  filter(s1 != 0) %>% 
  arrange(-abs(s1))) -> ridge_coefs

print('Ridge 200 predictors arranged by absolute value of coefficient:')
ridge_coefs[3]

# write to file for ridge models as well
write_rds(ridge_coefs, "../output/ridge_coefs.rds")
```

Conclusion: 
- The ridge model with 200 ASVs has less complexity with a comparable error rate in this analysis. 

- The 
