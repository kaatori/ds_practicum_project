---
title: "RandomForest"
author: "Cassandra Sperow"
date: "2023-11-09"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
suppressMessages(library(tidyverse))
```

# Random Forest Modeling of Clade C Coral Data
- Logistic regression has proven to not be a good method of modeling the CLade C data. The data are inherently non-linear and the ASVs have many dependencies presenting multicollinearity. 

- To obtain an index of the ASVs or variables that define Clade C the most, a non-parametric method such as RandomForest is useful and allows for interpretability of the model with its outut of an importance list. 

## Read in Clade C and Non-Clade C Balanced dataset with CLR-Transformed ASV data
- The lr_clr_df_2.csv file also inlcudes other variables such as species and region

```{r}
read_csv("../output/lr_clr_df_2.csv") %>% 
  mutate(ITS2_type = as.factor(ITS2_type), 
         Clade = as.factor(Clade)) -> df
```

- Need to take out the ITS2 Type as this would be defining Clade C and would be highly correlated.
```{r}
library(randomForest)
set.seed(123)
BAG_t <- randomForest(Clade ~ . ,
                     data = df[,-c(1,3)])

BAG_t
```
- Error in having so many categories with ITS2 type; removing to re-run model.
- With taking out the ITS2 type column and the sample ID column, the random forest model gives an error rate of about 29 % overall. 

```{r}
plot(BAG_t)
```


```{r}
BAG_t$err.rate[which.min(BAG_t$err.rate)]
```


```{r}
############################## optimal number of trees
(BAG_t$err.rate |> as.data.frame())$OOB |> which.min() # 292 trees
```


```{r}
############################## find optimal mtry
set.seed(3)
# training set
Zt <- sample(nrow(df), .8*nrow(df))
tuneRF(df[Zt,-c(1,3)], df[Zt,]$Clade, mtryStart = 2) # 
```
## Re-run with Optimal Number of Trees and 'mtry' Splits

```{r}
set.seed(123)
BAG_8 <- randomForest(Clade ~ . ,
                     data = df[,-c(1,3)], 
                     mtry = 8, 
                     ntree = 292)

BAG_8
```


```{r}
importance(BAG_8) %>% 
  as.data.frame() %>% 
  arrange(-MeanDecreaseGini)
```

# Try again without the ITS2 count as it would also be highly correlated with Clade C. 

```{r}
library(randomForest)
set.seed(123)
BAG_t <- randomForest(Clade ~ . ,
                     data = df[,-c(1,3:4)])

BAG_t
```

- The error rate increases rather than decreases when ITS2 count is taken out. 

```{r}
plot(BAG_t)
```

- Seems a lot less stable

```{r}
BAG_t$err.rate[which.min(BAG_t$err.rate)]
```
- Error rate does not improve

```{r}
############################## optimal number of trees
(BAG_t$err.rate |> as.data.frame())$OOB |> which.min() # 123 trees
```


```{r}
############################## find optimal mtry
set.seed(3)
# training set
Zt <- sample(nrow(df), .8*nrow(df))
tuneRF(df[Zt,-c(1,3:4)], df[Zt,]$Clade, mtryStart = 2) # 
```
## Re-run with Optimal Number of Trees and 'mtry' Splits

```{r}
set.seed(123)
BAG_4 <- randomForest(Clade ~ . ,
                     data = df[,-c(1,3:4)], 
                     mtry = 4, 
                     ntree = 123)

BAG_4
```

- The error rate for clade C goes down while the error rate for non-clade c goes up to a coin toss.

- Compare the Importance breakdowns for each model
```{r}
importance(BAG_4) %>% 
  as.data.frame() %>% 
  arrange(-MeanDecreaseGini)


## from model with ITS2 count left in
importance(BAG_8) %>% 
  as.data.frame() %>% 
  arrange(-MeanDecreaseGini)
```

# To see if a better error rate can be possible, take original data and subset first 1000
```{r}
read_csv( "../output/lr_data.csv") %>% 
  mutate(Clade = as.factor(Clade))-> lr_data
```


```{r}
head(lr_data)
```

### Subset first 1000 and redo CLR transformation to see if error rate can improve
```{r}
library(compositions)
lr_data[,1:1002] -> lr_data_1000
 
  
bind_cols(lr_data_1000[,1:2], 
          lr_data_1000[,-c(1,2)] %>% clr()
            ) -> lr_clr_df_1000

head(lr_clr_df_1000)
```


```{r}
library(randomForest)
set.seed(123)
BAG_t_1000 <- randomForest(lr_clr_df_1000$Clade ~ . ,
                     data = lr_clr_df_1000[,-c(1,2)])

BAG_t_1000
```

- The error rate did not improve when only using 1000 ASVs from PCA.

## What about top 500 
```{r}
lr_data[,1:502] -> lr_data_500
 
  
bind_cols(lr_data_500[,1:2], 
          lr_data_500[,-c(1,2)] %>% clr()
            ) -> lr_clr_df_500

head(lr_clr_df_500)
```


```{r}
set.seed(123)
BAG_t_500 <- randomForest(lr_clr_df_500$Clade ~ . ,
                     data = lr_clr_df_500[,-c(1,2)])

BAG_t_500
```

- The error rate increased again higher. 

```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```









