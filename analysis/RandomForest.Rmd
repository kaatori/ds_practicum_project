---
title: "RandomForest"
author: "Cassandra Sperow"
date: "2023-11-09"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
suppressMessages(library(tidyverse))
```

# Experimental Random Forest Modeling of Clade C Coral Data
- Logistic regression has proven to not be a good method of modeling the CLade C data. The data are inherently non-linear and the ASVs have many dependencies presenting multicollinearity. 

- To obtain an index of the ASVs or variables that define Clade C the most, a non-parametric method such as RandomForest is useful and allows for interpretability of the model with its outut of an importance list. 

## Read in Clade C and Non-Clade C Balanced dataset with CLR-Transformed ASV data
- The lr_clr_df_2.csv file also inlcudes other variables such as species and region

```{r}
read_csv("../output/lr_clr_df_2.csv") %>% 
  mutate(ITS2_type = as.factor(ITS2_type), 
         Clade = as.factor(Clade)) -> df
```

- Need to take out the ITS2 Type as this would be defining Clade C and would be highly correlated.
```{r}
library(randomForest)
set.seed(123)
BAG_t <- randomForest(Clade ~ . ,
                     data = df[,-c(1,3)])

BAG_t
```
- Error in having so many categories with ITS2 type; removing to re-run model.
- With taking out the ITS2 type column and the sample ID column, the random forest model gives an error rate of about 29 % overall. 

```{r}
base::plot(BAG_t)
```


```{r}
BAG_t$err.rate[which.min(BAG_t$err.rate)]
```


```{r}
############################## optimal number of trees
(BAG_t$err.rate |> as.data.frame())$OOB |> which.min() # 292 trees
```


```{r}
############################## find optimal mtry
set.seed(3)
# training set
Zt <- sample(nrow(df), .8*nrow(df))
tuneRF(df[Zt,-c(1,3)], df[Zt,]$Clade, mtryStart = 2) # 
```
## Re-run with Optimal Number of Trees and 'mtry' Splits

```{r}
set.seed(123)
BAG_8 <- randomForest(Clade ~ . ,
                     data = df[,-c(1,3)], 
                     mtry = 8, 
                     ntree = 292)

BAG_8
```


```{r}
importance(BAG_8) %>% 
  as.data.frame() %>% 
  arrange(-MeanDecreaseGini)
```



### Try again without the ITS2 count as it would also be highly correlated with Clade C. 

```{r}
library(randomForest)
set.seed(123)
BAG_t <- randomForest(Clade ~ . ,
                     data = df[,-c(1,3:4)])

BAG_t
```

- The error rate increases rather than decreases when ITS2 count is taken out. 

```{r}
base::plot(BAG_t)
```

- Seems a lot less stable

```{r}
BAG_t$err.rate[which.min(BAG_t$err.rate)]
```
- Error rate does not improve

```{r}
############################## optimal number of trees
(BAG_t$err.rate |> as.data.frame())$OOB |> which.min() # 123 trees
```


```{r}
############################## find optimal mtry
set.seed(3)
# training set
Zt <- sample(nrow(df), .8*nrow(df))
tuneRF(df[Zt,-c(1,3:4)], df[Zt,]$Clade, mtryStart = 2) # 
```
## Re-run with Optimal Number of Trees and 'mtry' Splits

```{r}
set.seed(123)
BAG_4 <- randomForest(Clade ~ . ,
                     data = df[,-c(1,3:4)], 
                     mtry = 4, 
                     ntree = 123)

BAG_4
```

- The error rate for clade C goes down while the error rate for non-clade c goes up to a coin toss.

- Compare the Importance breakdowns for each model
```{r}
importance(BAG_4) %>% 
  as.data.frame() %>% 
  arrange(-MeanDecreaseGini)


## from model with ITS2 count left in
importance(BAG_8) %>% 
  as.data.frame() %>% 
  arrange(-MeanDecreaseGini)
```


# ASVs Only 1000 from PCA Results for Clade C

## Read in original data with PCA results for Clade C as ordered ASV columns
```{r}
read_csv( "../output/lr_data.csv") %>% 
  mutate(Clade = as.factor(Clade))-> lr_data
```


```{r}
head(lr_data)
```
- The ASV columns are ordered in terms of the PCA results for Clade C from Dimension 1. 

### RandomForest 1000 ASVs

- CLR transformation recommended by client and used also in Ridge/Lasso Modeling
- To compare with Ridge/Lasso Modeling, these are the same along with 85 % training data.
```{r}
library(compositions)
lr_data[,1:1002] -> lr_data_1000
 
  
bind_cols(lr_data_1000[,1:2], 
          lr_data_1000[,-c(1,2)] %>% clr()
            ) -> lr_clr_df_1000

head(lr_clr_df_1000)
```


```{r}
library(randomForest)
set.seed(123)
BAG_t_1000 <- randomForest(lr_clr_df_1000$Clade ~ . , # clade is factor
                     data = lr_clr_df_1000[,-c(1)], # minus sample id
                     )

BAG_t_1000
base::plot(BAG_t_1000)

############################## optimal number of trees
(BAG_t_1000$err.rate |> as.data.frame())$OOB |> which.min() # 464
```

```{r}
############################## find optimal mtry
set.seed(3)
# training set
Z_1000 <- sample(nrow(lr_clr_df_1000), .85*nrow(lr_clr_df_1000)) # same % as lasso/ridge
tuneRF(lr_clr_df_1000[Z_1000,-c(1,2)], # x train minus sample id and clade columns
       lr_clr_df_1000[Z_1000,]$Clade, mtryStart = 2) # y train
```


## RandomForest 500 ASVs
- Redo CLR transformation
```{r}
lr_data[,1:502] -> lr_data_500
 
  
bind_cols(lr_data_500[,1:2], 
          lr_data_500[,-c(1,2)] %>% clr()
            ) -> lr_clr_df_500

head(lr_clr_df_500)
```


```{r}
set.seed(123)
BAG_t_500 <- randomForest(lr_clr_df_500$Clade ~ . , # clade is factor
                     data = lr_clr_df_500[,-c(1,2)])

BAG_t_500

```


- The error rate increased again higher. The default for ntree = 500 and it fit best with 500; therefore, increase ntree to 1000 to see if there can be a lower OOB error rate. 

```{r}
set.seed(123)
BAG_t_500_2 <- randomForest(lr_clr_df_500$Clade ~ . , # clade is factor
                     data = lr_clr_df_500[,-c(1,2)], 
                     ntree = 1000)

BAG_t_500_2
```
```{r}
base::plot(BAG_t_500_2)
```


```{r}
############################## optimal number of trees
(BAG_t_500_2$err.rate |> as.data.frame())$OOB |> which.min() # 986 trees
```

# Training & Testing Split
```{r}
############################## find optimal mtry
set.seed(3)
# training set
Z_500 <- sample(nrow(lr_clr_df_500), .85*nrow(lr_clr_df_500)) # same as ridge/lasso 85 %
tuneRF(lr_clr_df_500[Z_500,-c(1,2)], 
       lr_clr_df_500[Z_500,]$Clade, mtryStart = 2) 
```
 
 # Re-run 500 with Optimal ntree and mtry

```{r}
set.seed(123)
BAG_500_o <- randomForest(Clade ~ . , # clade is already a factor
                     data = lr_clr_df_500[,-c(1)], 
                     mtry = 2, # from above tuning 
                     ntree = 986) # from above

BAG_500_o
```



# Overall Error Rates 1000, 500:





