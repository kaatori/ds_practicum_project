---
title: "Ridge & Lasso Modeling of Clade C Bacteria"
author: "Cassandra Sperow"
date: "2023-11-14"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
suppressMessages(library(tidyverse))
```

#### Note: Revised to make sure to use ```cv.glmnet()``` with K-fold cross-validation when fitting the ridge and lasso models
#### The modeling that includes other variables with the ASVs returns better error rates. This section starts with the heading 'Other variables'

# Review Goal & Question(s)
- Can Clade C bacteria be modeled using classification algorithms?
- When Clade C bacteria are modeled, which bacteria strains (ASVs) seem to be the 'best', or most associated, predictors of Clade C?

# Import Logistic Regression Balanced Dataset
- Clade C: n=139
- Non-Clade C: n=139
- Use first 1000 ASVs obtained in PCA Dim.1
- The ```lr_data.csv``` dataset was created by subsetting the Clade C observations from the original 659 samples with random sampling from non-Clade C rows to obtain an equal number of observations for each response label of Clade C = 1, Non-Clade C = 0. This data subsetting process was obtained in the original modeling exploration file 'Logistic Regression Clade C.Rmd'. 
- The Clade C Principal Components Analysis results from dimension 1 were obtained in the file 'PCA-CLR.Rmd'.
- The ASV data are transformed for modeling with the centered log ratio of the raw abundance counts. This is redone when subsetting for the PCA 1000 results and again when subsetting the first 500 PCA results. 

```{r}
# read in balanced Clade C and non-Clade C observations 
# this was created from balanced df from previous file, but only has the ASV columns, Clade and Sample ID
read_csv( "../output/lr_data.csv") -> lr_data
# read in saved PCA ranks for Dimension 1
read_csv("../output/clade_c_pca_Ranks.csv") -> clade_c_pca_Ranks


  
```

```{r}
# order the Clade C ASVs by selecting from the vector of PCA results
# this is for subsetting the first 1000 or 500 in order of their PCA rank
lr_data %>% 
  select(sample_id, Clade, (clade_c_pca_Ranks$ASV)) %>% 
  select(1:1002) -> lr_data_1000

dim(lr_data_1000)
```

- Note that Clade as the Y response is a double as glmnet expects a numeric 0/1 for classifciation.
```{r}
#glimpse(lr_data)
head(lr_data_1000)
```

### Centered-Log-Ratio Data Transformation of 1000 ASV Numeric Columns
```{r}
library(compositions)
# CLR Data Transformation on only ASV numeric columns
bind_cols(lr_data_1000[,1:2], # sample id and Clade columns
          # re-do CLR transformation on relative first 1000 PCA
          lr_data_1000[,-c(1,2)] %>% clr() 
            ) -> lr_clr_df_1000

```

```{r}
head(lr_clr_df_1000)
#lr_clr_df_1000$ASV0001[1:5]
```



# Ridge 1000 

#### Training & CV 1000

- ```glmnet``` documentation:

https://glmnet.stanford.edu/articles/glmnet.html

**cv.glmnet()** performs K-fold cross-validation without the need for separating training/testing.

```{r}
library(glmnet)
# create model matrix for glmnet
#stats::model.matrix()

# Create a formula for model.matrix
formula <- as.formula(paste("Clade ~ .", sep = "", collapse = ""), 
                      env = environment(lr_clr_df_1000))

# Create the design matrix
# take out the sample id
X <- stats::model.matrix(formula, data = lr_clr_df_1000[,-c(1)])
dim(X)

y <- lr_clr_df_1000$Clade
length(y)


# Train the RIDGE model using K-fold cross-validation
set.seed(23)
RidgeCV_1000 <- cv.glmnet(X[,-1], # take out intercept
             y, 
             alpha = 0,  # RIDGE
             type.measure = "class", # misclassification error
             nfolds = 10,
             family = "binomial"
             )
```

### RR Plot
```{r}
base::plot(RidgeCV_1000)
```

-  Access the error rate
```{r}
RidgeCV_1000
#min(RidgeCV_1000$cvm) # from lambda.min 

```

```{r}
# best fitted model from CV with minimum error rate
coef(RidgeCV_1000, s = "lambda.min") %>% 
  as.matrix() %>% 
  as.data.frame() %>% 
  # order in terms of magnitude although beware ridge uses bias constraint
  # the magnitude should be interpreted in terms of the coef that were not shrunk as fast to 0
  arrange(-abs(s1)) %>% 
  head()
```

# Lasso 1000
- Use same design matrix as above with 1000 ASV columns
```{r}
dim(X)
length(y)
set.seed(123)
lasso_cv <- cv.glmnet(X[,-1], 
                   y, 
                   alpha = 1, # Lasso
                   type.measure = "class", # must have for classification
                   family = "binomial" # classification
                   )
lasso_cv

base::plot(lasso_cv)

# best lasso models from CV
lasso_cv
```


# Ridge 500 
### Data Prep
- Subset from original counts for the first 500 ASVs from PCA results
```{r}
# subset the first 500 
lr_data_1000[, 1:502] -> lr_data_500

# redo CLR transformation with counts for first 500
bind_cols(lr_data_500[,1:2], # 278 by 2
          lr_data_500[,-c(1,2)] %>% clr() # 278 by 500
            ) -> lr_clr_500
```


### Design matrix for 500 columns
```{r}
# Create a formula for model.matrix
formula <- as.formula(paste("Clade ~ .", sep = "", collapse = ""), 
                      env = environment(lr_clr_500))

# Create the design matrix
# take out the sample id
X <- stats::model.matrix(formula, data = lr_clr_500[,-c(1)])
dim(X)

y <- lr_clr_500$Clade
length(y)
```


#### Training & CV 500
```{r}
set.seed(42)
# Train the RIDGE model with 500 ASVs
RidgeCV_500 <- cv.glmnet(X[,-1], # take out intercept
                 y, 
                 alpha = 0,  # RIDGE
                 nfolds = 10,
                 type.measure = "class", # misclassification error
                 family = "binomial") 

# best lambda
base::plot(RidgeCV_500)
```


```{r}
RidgeCV_500
```


```{r}
coef(RidgeCV_500, s = "lambda.min") %>% 
  as.matrix() %>% 
  as.data.frame() %>% 
  # ordering in terms of magnitude
  arrange(-abs(s1)) %>% 
  head()
```

# What if it was 200 ASVs?
- PCA from Clade C had about 150 or less that created a plateau in one of the plots for PCA top contributing ASVs. See the Rmd file for PCA. 
```{r}
# subset the first 200
lr_data_1000[, 1:202] -> lr_data_200

# redo CLR transformation with counts for first 200
bind_cols(lr_data_200[,1:2], #  278 by 2
          lr_data_200[,-c(1,2)] %>% clr() # 278 by 200
            ) -> lr_clr_200

####################################### Design Matrix
# Create a formula for model.matrix
formula <- as.formula(paste("Clade ~ .", sep = "", collapse = ""), 
                      env = environment(lr_clr_200))

# Create the design matrix
# take out the sample id
X <- stats::model.matrix(formula, data = lr_clr_200[,-c(1)])
dim(X)

y <- lr_clr_200$Clade
length(y)

##################################### Training and CV for 200 ASVs
set.seed(23)
RidgeCV_200 <- cv.glmnet(X[,-1], # take out intercept
                 y, 
                 alpha = 0,  # RIDGE
                 nfolds = 10,
                 type.measure = "class", # misclassification error
                 family = "binomial") 

# best lambda
base::plot(RidgeCV_200)

```

```{r}
RidgeCV_200
```

- Is the error rate going up because maybe fewer ASVs make the data even more sparse?

- It does not seem to be helping to redo with cv.glmnet when the error rate is going up. 


# Other Variables
- Including other variables from original data to see if it can give better error rate
```{r redo-data-set}
# read in balanced df for clade C with other variables such as region and ITS2 count from algae of Clade C
read_csv("../output/balanced_df.csv") %>% 
  # take out reef because it is not specific
  # take out Majority because it is used to define Clade C - as well as ITS2 Type
  select(-reef, -Majority, -ITS2_type) -> balanced_w_other_vars

# make dataset for 1000 ASVs 
balanced_w_other_vars %>% 
  # reorder for only CLade C PCA ranks of ASVs
  select(Clade, sample_id, its2_count, species, region, c(clade_c_pca_Ranks$ASV[1:1000])) %>% 
  mutate(Clade = ifelse(Clade=="C", 1, 0)) -> balanced_1000
# clr for 1000
bind_cols(
  balanced_1000[,1:5],
  clr(balanced_1000[,6:1005]) 
) -> clr_1000


head(clr_1000)
dim(clr_1000)

write_csv(clr_1000, "../output/clr_1000.csv")

# make dataset for the 500 ASVs
balanced_w_other_vars %>% 
  select(Clade, sample_id, its2_count, species, region, c(clade_c_pca_Ranks$ASV[1:500])) %>% 
  mutate(Clade = ifelse(Clade=="C", 1, 0)) -> balanced_500
# clr for 500
bind_cols(
  balanced_500[,1:5],
  clr(balanced_500[,6:505])
) -> clr_500

head(clr_500)

write_csv(clr_500, "../output/clr_500.csv")



# make another for the first 200 ASVs
balanced_w_other_vars %>% 
  select(Clade, sample_id, its2_count, species, region, c(clade_c_pca_Ranks$ASV[1:200])) %>% 
  mutate(Clade = ifelse(Clade=="C", 1, 0)) -> balanced_200
# clr for 200
bind_cols(
  balanced_500[,1:5],
  clr(balanced_500[,6:205])
) -> clr_200

head(clr_200)
write_csv(clr_200, "../output/clr_200.csv")

```

# 1000
```{r}
# design matrix
library(glmnet)
# Create a formula for model.matrix
formula <- as.formula(paste("Clade ~ .", sep = "", collapse = ""), 
                      env = environment(clr_1000))

X <- stats::model.matrix(formula, data = clr_1000[,-c(2)]) # minus sample id
dim(X)
#X[,1:10]

y <-  clr_1000$Clade
length(y)

# Fit the RIDGE model using K-fold cross-validation
set.seed(23)
R_clr_1000 <- cv.glmnet(X[,-1], # take out intercept
             y, 
             alpha = 0,  # RIDGE
             type.measure = "class", # misclassification error
             nfolds = 10,
             family = "binomial"
             )

# call
R_clr_1000 # 28.4 % with lambda min

# plot
base::plot(R_clr_1000)

# coef
coef(R_clr_1000, s = "lambda.min") %>% 
  as.matrix() %>% 
  as.data.frame() %>% 
  arrange(-abs(s1)) %>% 
  head()
```

# 500
```{r}
# Create a formula for model.matrix
formula <- as.formula(paste("Clade ~ .", sep = "", collapse = ""), 
                      env = environment(clr_500))

X <- stats::model.matrix(formula, data = clr_500[,-c(2)]) # minus sample id
dim(X)
# X[,1:10]

y <-  clr_500$Clade
length(y)

# Fit the RIDGE model using K-fold cross-validation
set.seed(23)
R_clr_500 <- cv.glmnet(X[,-1], # take out intercept
             y, 
             alpha = 0,  # RIDGE
             type.measure = "class", # misclassification error
             nfolds = 10,
             family = "binomial"
             )

# call
R_clr_500 #    % with lambda min

# plot
base::plot(R_clr_500)

# coef
coef(R_clr_500, s = "lambda.min") %>% 
  as.matrix() %>% 
  as.data.frame() %>% 
  arrange(-abs(s1)) %>% 
  head()
```

# 200 
```{r}
# Create a formula for model.matrix
formula <- as.formula(paste("Clade ~ .", sep = "", collapse = ""), 
                      env = environment(clr_200)) # 200

X <- stats::model.matrix(formula, data = clr_200[,-c(2)]) # minus sample id for 200 ASVs with other vars
dim(X)
# X[,1:10]

y <-  clr_200$Clade # 200
length(y)

# Fit the RIDGE model using K-fold cross-validation
set.seed(23)
R_clr_200 <- cv.glmnet(X[,-1], # take out intercept
             y, 
             alpha = 0,  # RIDGE
             type.measure = "class", # misclassification error
             nfolds = 10,
             family = "binomial"
             )

# call
R_clr_200 #    % with lambda min

# plot
base::plot(R_clr_200)

# coef
coef(R_clr_200, s = "lambda.min") %>% 
  as.matrix() %>% 
  as.data.frame() %>% 
  arrange(-abs(s1)) %>% 
  head()

```

- Wow, the error rate went down drastically to 16.2 % for lambda.min with a standard error for lambda.min that is slightly larger. This was using more than just the ASV columns. The model includes region, species, and ITS2_count. 

# Lasso Modeling with other variables included
# Lasso 1000
```{r}
# design matrix
library(glmnet)
# Create a formula for model.matrix
formula <- as.formula(paste("Clade ~ .", sep = "", collapse = ""), 
                      env = environment(clr_1000))

X <- stats::model.matrix(formula, data = clr_1000[,-c(2)]) # minus sample id
dim(X)
# X[,1:10]

y <-  clr_1000$Clade
length(y)

############### Fit the LASSO model using K-fold cross-validation
set.seed(23)
L_clr_1000 <- cv.glmnet(X[,-1], # take out intercept
             y, 
             alpha = 1,  # LASSO alpha = 1 
             type.measure = "class", # misclassification error
             nfolds = 10,
             family = "binomial"
             )

# call
L_clr_1000 #   % with lambda min

# plot
base::plot(L_clr_1000)

# coef
coef(L_clr_1000, s = "lambda.min") %>% 
  as.matrix() %>% 
  as.data.frame() %>% 
  filter(s1 != 0) %>% 
  arrange(-abs(s1)) %>% 
  head()
```

# Lasso 500
```{r}
# Create a formula for model.matrix
formula <- as.formula(paste("Clade ~ .", sep = "", collapse = ""), 
                      env = environment(clr_500))

X <- stats::model.matrix(formula, data = clr_500[,-c(2)]) # minus sample id
dim(X)
# X[,1:10]

y <-  clr_500$Clade
length(y)

############### Fit the LASSO model using K-fold cross-validation
set.seed(23)
L_clr_500 <- cv.glmnet(X[,-1], # take out intercept
             y, 
             alpha = 1,  # LASSO alpha = 1 
             type.measure = "class", # misclassification error
             nfolds = 10,
             family = "binomial"
             )

# call
L_clr_500 #   % with lambda min

# plot
base::plot(L_clr_500)

# coef
coef(L_clr_500, s = "lambda.min") %>% 
  as.matrix() %>% 
  as.data.frame() %>% 
  filter(s1 != 0) %>% 
  arrange(-abs(s1)) %>% 
  head()
```

# Lasso 200
```{r}
# Create a formula for model.matrix
formula <- as.formula(paste("Clade ~ .", sep = "", collapse = ""), 
                      env = environment(clr_200))

X <- stats::model.matrix(formula, data = clr_200[,-c(2)]) # minus sample id
dim(X)
# X[,1:10]

y <-  clr_200$Clade
length(y)

############### Fit the LASSO model using K-fold cross-validation
set.seed(23)
L_clr_200 <- cv.glmnet(X[,-1], # take out intercept
             y, 
             alpha = 1,  # LASSO alpha = 1 
             type.measure = "class", # misclassification error
             nfolds = 10,
             family = "binomial", 
             # maxit = 10^7 ##### this is needed because of a warning message where it ran out before doing 100 lambda values
             )

# call
L_clr_200 #   % with lambda min

# plot
base::plot(L_clr_200)

# coef
coef(L_clr_200, s = "lambda.min") %>% 
  as.matrix() %>% 
  as.data.frame() %>% 
  filter(s1 != 0) %>% 
  arrange(-abs(s1)) %>% 
  head()
```

# Overall Rates for Models with Other Variables

```{r}
#min(R_clr_1000$cvm)

# save outputs into list for using map()
list(
R_clr_1000,
R_clr_500,
R_clr_200) -> list_ridge_models

map(list_ridge_models, ~min(.$cvm)) %>% 
  as_vector() -> ridge_error_rates

# save outputs into list for using map()
list(L_clr_1000,
L_clr_500,
L_clr_200
) -> list_lasso_models

# map
map(list_lasso_models, ~min(.$cvm)) %>% 
  as_vector() -> lasso_error_rates

model_names <- c("ridge 1000", "ridge 500", "ridge 200", "lasso 1000", "lasso 500", "lasso 200")

# combine to have them all in ore place for comparison
cbind(
  model_names,
c(ridge_error_rates, lasso_error_rates)
) %>% 
  as.data.frame() %>% 
  mutate(error_rates = V2) %>% 
  mutate(error_rates = as.numeric(error_rates)) %>% 
  arrange(error_rates)

```

- The models with other variables perform better overall. The lowest error rate obtained in the first 'Ridge & Lasso Modeling.Rmd' file was 29 %. Here there are several models with error rates around 15-16 %, a dramatic improvement. 

- For the variables that were selected from the lasso model with the lowest error rate, the recommendation would be to see if this makes biologic sense for the client as the subject matter expert, for example, if she is familiar with any of the ASVs that come up as predictors. 

```{r}
L_clr_1000
L_clr_500
L_clr_200

map(list_lasso_models, ~coef(., s = "lambda.min") %>% as.matrix(.) %>% 
      as.data.frame() %>% 
      mutate(predictor = rownames(.)) %>% 
  filter(s1 != 0) %>% 
  arrange(-abs(s1))) -> lasso_coefs


# test
#str(lasso_coefs[1])
# coef(L_clr_200, s = "lambda.min") %>% 
#   as.matrix() %>% 
#   as.data.frame() %>% 
#   filter(s1 != 0) %>% 
#   arrange(-abs(s1))

lasso_coefs
```

```{r}
write_rds(lasso_coefs, "../output/lasso_coefs.rds")
```

### Region MAQ in all of Lasso Variable Selections
```{r}

clr_200 %>% 
  filter(Clade == "1") %>% 
  group_by(region) %>% 
  summarise(n = n()) %>% 
  arrange(-n)
```


