---
title: "RandomForest"
author: "Cassandra Sperow"
date: "2023-11-09"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
suppressMessages(library(tidyverse))
```

# Experimental Random Forest Modeling of Clade C Coral Data
- Logistic regression has proven to not be a good method of modeling the CLade C data. The data are inherently non-linear and the ASVs have many dependencies presenting multicollinearity. 

- To obtain an index of the ASVs or variables that define Clade C the most, a non-parametric method such as RandomForest is useful and allows for interpretability of the model with its outut of an importance list. 

## Read in Clade C and Non-Clade C Balanced dataset with CLR-Transformed ASV data
- The lr_clr_df_2.csv file also inlcudes other variables such as species and region

```{r}
read_csv("../output/lr_clr_df_2.csv") %>% 
  mutate(ITS2_type = as.factor(ITS2_type), 
         Clade = as.factor(Clade)) -> df
```

- Need to take out the ITS2 Type as this would be defining Clade C and would be highly correlated.
```{r}
library(randomForest)
set.seed(123)
BAG_t <- randomForest(Clade ~ . ,
                     data = df[,-c(1,3)])

BAG_t
```
- Error in having so many categories with ITS2 type; removing to re-run model.
- With taking out the ITS2 type column and the sample ID column, the random forest model gives an error rate of about 29 % overall. 

```{r}
base::plot(BAG_t)
```


```{r}
BAG_t$err.rate[which.min(BAG_t$err.rate)]
```


```{r}
############################## optimal number of trees
(BAG_t$err.rate |> as.data.frame())$OOB |> which.min() # 292 trees
```


```{r}
############################## find optimal mtry
set.seed(3)
# training set
Zt <- sample(nrow(df), .8*nrow(df))
tuneRF(df[Zt,-c(1,3)], df[Zt,]$Clade, mtryStart = 2) # 
```
## Re-run with Optimal Number of Trees and 'mtry' Splits

```{r}
set.seed(123)
BAG_8 <- randomForest(Clade ~ . ,
                     data = df[,-c(1,3)], 
                     mtry = 8, 
                     ntree = 292)

BAG_8
```


```{r}
importance(BAG_8) %>% 
  as.data.frame() %>% 
  arrange(-MeanDecreaseGini)
```



### Try again without the ITS2 count as it would also be highly correlated with Clade C. 

```{r}
library(randomForest)
set.seed(123)
BAG_t <- randomForest(Clade ~ . ,
                     data = df[,-c(1,3:4)])

BAG_t
```

- The error rate increases rather than decreases when ITS2 count is taken out. 

```{r}
base::plot(BAG_t)
```

- Seems a lot less stable

```{r}
BAG_t$err.rate[which.min(BAG_t$err.rate)]
```
- Error rate does not improve

```{r}
############################## optimal number of trees
(BAG_t$err.rate |> as.data.frame())$OOB |> which.min() # 123 trees
```


```{r}
############################## find optimal mtry
set.seed(3)
# training set
Zt <- sample(nrow(df), .8*nrow(df))
tuneRF(df[Zt,-c(1,3:4)], df[Zt,]$Clade, mtryStart = 2) # 
```
## Re-run with Optimal Number of Trees and 'mtry' Splits

```{r}
set.seed(123)
BAG_4 <- randomForest(Clade ~ . ,
                     data = df[,-c(1,3:4)], 
                     mtry = 4, 
                     ntree = 123)

BAG_4
```

- The error rate for clade C goes down while the error rate for non-clade c goes up to a coin toss.

- Compare the Importance breakdowns for each model
```{r}
importance(BAG_4) %>% 
  as.data.frame() %>% 
  arrange(-MeanDecreaseGini)


## from model with ITS2 count left in
importance(BAG_8) %>% 
  as.data.frame() %>% 
  arrange(-MeanDecreaseGini)
```


# ASVs Only 1000 from PCA Results for Clade C

## Read in original data with PCA results for Clade C as ordered ASV columns
```{r}
read_csv( "../output/lr_data.csv") %>% 
  mutate(Clade = as.factor(Clade))-> lr_data
```


```{r}
head(lr_data)
```
- The ASV columns are ordered in terms of the PCA results for Clade C from Dimension 1. 

### RandomForest 1000 ASVs

- CLR transformation recommended by client and used also in Ridge/Lasso Modeling
- To compare with Ridge/Lasso Modeling, these are the same along with 85 % training data and the format of the data frame.
```{r}
library(compositions)
lr_data[,1:1002] -> lr_data_1000
 
  
bind_cols(lr_data_1000[,1:2], 
          lr_data_1000[,-c(1,2)] %>% clr()
            ) -> lr_clr_df_1000

head(lr_clr_df_1000)
```


```{r}
library(randomForest)
set.seed(123)
BAG_t_1000 <- randomForest(lr_clr_df_1000$Clade ~ . , # clade is factor
                     data = lr_clr_df_1000[,-c(1,2)], # minus sample id and clade
                     )

BAG_t_1000
base::plot(BAG_t_1000)

############################## optimal number of trees
(BAG_t_1000$err.rate |> as.data.frame())$OOB |> which.min() # 464
```

# Tune RF with Training & Testing Split for 1000
```{r}
############################## find optimal mtry
set.seed(3)
# training set
Z_1000 <- sample(nrow(lr_clr_df_1000), .85*nrow(lr_clr_df_1000)) # same % as lasso/ridge
tuneRF(lr_clr_df_1000[Z_1000,-c(1,2)], # x train minus sample id and clade columns
       lr_clr_df_1000[Z_1000,]$Clade, mtryStart = 2) # y train
```

## Re-fit 1000 with optimal ntree and mtry
```{r}
set.seed(123)
BAG_1000_o <- randomForest(lr_clr_df_1000$Clade ~ . , # clade is already a factor
                     data = lr_clr_df_1000[,-c(1,2)], # minus sample id and clade
                     mtry = 2, # from above tuning 
                     ntree = 464) # from above

BAG_1000_o
```



## RandomForest 500 ASVs
- Redo CLR transformation
```{r}
lr_data[,1:502] -> lr_data_500
 
  
bind_cols(lr_data_500[,1:2], 
          lr_data_500[,-c(1,2)] %>% clr()
            ) -> lr_clr_df_500

head(lr_clr_df_500)
```


```{r}
set.seed(123)
BAG_t_500 <- randomForest(lr_clr_df_500$Clade ~ . , # clade is factor
                     data = lr_clr_df_500[,-c(1,2)])

BAG_t_500

```


- The error rate increased again higher. The default for ntree = 500 and it fit best with 500; therefore, increase ntree to 1000 to see if there can be a lower OOB error rate. 

```{r}
set.seed(123)
BAG_t_500_2 <- randomForest(lr_clr_df_500$Clade ~ . , # clade is factor
                     data = lr_clr_df_500[,-c(1,2)], 
                     ntree = 1000)

BAG_t_500_2
```
```{r}
base::plot(BAG_t_500_2)
```


```{r}
############################## optimal number of trees
(BAG_t_500_2$err.rate |> as.data.frame())$OOB |> which.min() # 986 trees
```

# Tune RF with Training & Testing Split 500 
```{r}
############################## find optimal mtry
set.seed(3)
# training set
Z_500 <- sample(nrow(lr_clr_df_500), .85*nrow(lr_clr_df_500)) # same as ridge/lasso 85 %
tuneRF(lr_clr_df_500[Z_500,-c(1,2)], 
       lr_clr_df_500[Z_500,]$Clade, mtryStart = 2) 
```
 
### Re-run 500 with Optimal ntree and mtry

```{r}
set.seed(123)
BAG_500_o <- randomForest(Clade ~ . , # clade is already a factor
                     data = lr_clr_df_500[,-c(1)], 
                     mtry = 2, # from above tuning 
                     ntree = 986) # from above

BAG_500_o
```



# Overall RandomForest Error Rates 1000, 500:

```{r}
BAG_500_o
BAG_1000_o
```


- Both optimized RandomForest models resulted in a 37 % error rate with the 'mtry' set at 2 variables. This may be due to the necessity of leaving the two species together, similar to K-means at times finding 2 clusters before the split of the data into separate species. Recall that the Clade C observations were n = 139, and they were kept together to increase the number of rows of data for modeling Clade C despite species. 

- Also of note is that the individual class error rates for class 0 are worse than a coin toss (65-66 % errors). 

- RandomForest modeling was chosen as an experiment to see if its non-parametric nature would model these data better than Ridge or Lasso classification. Ridge and Lasso have better error rates than RandomForest under the above seeds. 

- Future research would attempt to increase the number of clade C observations in one species at a time with other non-clade C observations of equal number. Clade C was the 2nd most dominant Clade in the overall data. Clade A was the majority in terms of number of observations. See other analysis files in this directory, such as species split and files with 'EDA' in the titles. 


# Boosted Trees
```{r}
library(gbm)
```

# Boosted 1000
```{r}
# gbm needs integer not factor
lr_clr_df_1000 %>% 
  mutate(Clade = as.integer(Clade)-1) -> boost_data


set.seed(123)
boost_model_1000 <- gbm(boost_data[Z_1000,]$Clade ~ . , 
              data = boost_data[Z_1000, -c(1)], 
              n.trees = 5000, 
              distribution = "bernoulli") # logistic classification
boost_model_1000
summary(boost_model_1000)

```

```{r}

pred_test_1000 = predict.gbm(object = boost_model_1000,
                   newdata = boost_data[-Z_1000, -c(1)], # 42 rows of testing 
                   n.trees = 500,           # 500 tress to be built
                   type = "response")

pred_test_1000 # probabilities for bernoulli


```

```{r}
# classes from above probabilities
as.data.frame(pred_test_1000) %>% 
  mutate(actual_class = boost_data[-Z_1000, ]$Clade) %>% # y_train
  mutate(pred_class = ifelse(pred_test_1000 > 0.5, 1, 0)) -> pred_df

paste("Confusion Matrix Boosted Trees Model: ")
table(pred_df$actual_class, 
      pred_df$pred_class)


paste("Correct Classification Rate, Boosted Trees:  ")
((16+12)/42)*100
```

# Boosted 500

```{r}
# gbm needs integer not factor
lr_clr_df_500 %>% 
  mutate(Clade = as.integer(Clade)-1) -> boost_data_500


set.seed(123)
boost_model_500 <- gbm(boost_data_500[Z_500,]$Clade ~ . , 
              data = boost_data_500[Z_500, -c(1)], 
              n.trees = 5000, 
              distribution = "bernoulli") # logistic classification
boost_model_500
summary(boost_model_500)

```

```{r}

pred_test_500 = predict.gbm(object = boost_model_500,
                   newdata = boost_data_500[-Z_500, -c(1)], # 42 rows of testing 
                   n.trees = 500,           # 500 tress to be built
                   type = "response")

pred_test_500 # probabilities for bernoulli


```

```{r}
# classes from above probabilities
as.data.frame(pred_test_500) %>% 
  mutate(actual_class = boost_data[-Z_500, ]$Clade) %>% # y_train
  mutate(pred_class = ifelse(pred_test_500 > 0.5, 1, 0)) -> pred_500_df

paste("Confusion Matrix Boosted Trees Model: ")
table(pred_500_df$actual_class, 
      pred_500_df$pred_class)


paste("Correct Classification Rate, 500 ASVs, Boosted Trees:  ")
((16+9)/42)*100

paste("Error rate for Boosted 500 ASV model: ", 
      round(1 - ((16+9)/42), 3)
      )
```

